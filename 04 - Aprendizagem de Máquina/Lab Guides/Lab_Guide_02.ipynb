{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab Guide 02.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsLqqUd8escA",
        "colab_type": "text"
      },
      "source": [
        "# Lab Guide 02 - Aprendizagem de Máquina\n",
        "Nesse experimento, iremos trabalhar com o problema de regressão sobre o dataset clássico *The Boston Housing Dataset*, conhecido como Boston house-price. Veja mais informações [aqui](https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html).\n",
        "\n",
        "Ao completar essa tarefa, você:\n",
        "* Conhecerá, ainda que superficialmente, diversas bibliotecas Python para Ciência de Dados e Aprendizagem de Máquina;\n",
        "* Carregará um conjunto de dados para problema de regressão;\n",
        "* Terá noções básicas de visualização e normalização de conjuntos de dados;\n",
        "* Treinará diversos modelos de regressão sobre esse dataset; e\n",
        "* Visualizará o resultado produzido pelo seu modelo.\n",
        "\n",
        "**NOTA:** esse Lab Guide é baseado no material da Huawei, sendo disponibilizado para uso exclusivo pelos alunos do curso e não deve ser divulgado sem autorização prévia e expressa.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL-GiiYwmzef",
        "colab_type": "text"
      },
      "source": [
        "## [Passo 01] Carregando as Dependências\n",
        "\n",
        "Nesse passo iremos carregar diversas bibliotecas para auxiliar em Tarefas de aprendizagem de Máquina."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co7Z2IFweqXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filtragem dos avisos, pois se trata de um notebook onde o foco é resultado.\n",
        "# Aplicações devem evitar isso para não perder evidências úteis de erros.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# suporte a arrays e matrizes multidimensionais\n",
        "import numpy as np\n",
        "# plotagem semelhante ao MATLAB\n",
        "import matplotlib as mpl\n",
        "# diversas funções de plotagem\n",
        "import matplotlib.pyplot as plt \n",
        "# manipulação e análise de dados, tais como tabelas e séries temporais\n",
        "import pandas as pd\n",
        "# distribuições de probabilidade e funções estatísticas\n",
        "import scipy.stats as st\n",
        "# visualização de dados em alto nível\n",
        "import seaborn as sns\n",
        "\n",
        "# padronização de características: média zero e variância unitária\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# divisão de arrays e matrizes em subconjuntos, útil para cross validation\n",
        "from sklearn.model_selection import train_test_split\n",
        "# busca em gradem, útil para seleção de hiperparâmetros\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# coeficiente de determinação para regressão\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# carregamento do dataset Boston House Prices\n",
        "from sklearn.datasets import load_boston\n",
        "# modelos para regressãp\n",
        "from sklearn.linear_model import RidgeCV, LassoCV, LinearRegression, ElasticNet\n",
        "# SVM para problemas de regressão\n",
        "from sklearn.svm import SVR\n",
        "# algoritmos ensemble (comitê de modelos) para regressão\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "# algoritmo Extreme Gradient Boosting, do tipo ensemble\n",
        "from xgboost import XGBRegressor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCe3lB4SmqIL",
        "colab_type": "text"
      },
      "source": [
        "## [Passo 02] Carregando e Visualizando o Dataset\n",
        "\n",
        "A função utilitária [sklearn.datasets.load_boston()](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html?highlight=load_boston#sklearn.datasets.load_boston) carrega o dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAFbpBLXnQ6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boston = load_boston()\n",
        "#x features, and y labels.\n",
        "x = boston.data\n",
        "y = boston.target\n",
        "\n",
        "#Display related attributes.\n",
        "print('Feature column name')\n",
        "print(boston.feature_names)\n",
        "print('Sample data volume: %d, number of features: %d'% x.shape)\n",
        "print('Target sample data volume: %d'% y.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJn3cBCeCHBj",
        "colab_type": "text"
      },
      "source": [
        "Converte o dataset para um objeto *DataFrame* da biblioteca Pandas, o que facilita sua visualização e tratamento dos dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCoyeogyCMUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "x.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HhqqmfgCTip",
        "colab_type": "text"
      },
      "source": [
        "Visualização da distribuição dos rótulos *sem* estimativa de densidade por kernel e curva normal para ajuste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuY5fAhICWi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "sns.distplot( tuple(y), kde=False, fit=st.norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86ouzCvGECZo",
        "colab_type": "text"
      },
      "source": [
        "Agora iremos particionar as amostras do dataset, de modo que sejam usados dados distintos para treinamento e para teste. Isso permite avaliar o quanto o modelo de regressão é capaz de generalizar a partir dos dados observados no treinamento.\n",
        "\n",
        "Note que *random_state* é usado para definir um particionamento previsível para fins da atividade neste notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap6xTzw2D_QL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 80% das amostras são usadas para treinamento, 20% para teste\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=28)\n",
        "\n",
        "# padronização dos vetores característicos (features)\n",
        "ss = StandardScaler()\n",
        "x_train = ss.fit_transform(x_train)\n",
        "x_test = ss.transform(x_test)\n",
        "\n",
        "# visualização da subdivisão\n",
        "plt.figure()\n",
        "sns.distplot( tuple(y_train), kde=False, fit=st.norm, label='Train')\n",
        "plt.figure()\n",
        "sns.distplot( tuple(y_test), kde=False, fit=st.norm, label='Test')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWW6jAsx5yyC",
        "colab_type": "text"
      },
      "source": [
        "## [Passo 03] Usando Técnicas de Regressão\n",
        "Uma vez que os dados estejam separados, podemos lançar mão de diversos métodos de Aprendizagem de Máquina. A interface padronizada do pacote **sklearn** é bastante útil para esse propósito.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VB7IPhh466Az",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nomes identificando os modelos de regressão\n",
        "names = ['Linear',\n",
        "         'Ridge',\n",
        "         'Lasso',\n",
        "         'Random Forrest',\n",
        "         'GBDT',\n",
        "         'Support Vector Regression',\n",
        "         'ElasticNet',\n",
        "         'XgBoost'\n",
        "        ]\n",
        "\n",
        "# instancia diversos métodos de regressão correspondentes aos nomes\n",
        "models = [LinearRegression(),\n",
        "          RidgeCV(alphas=(0.001,0.1,1),cv=3),\n",
        "          LassoCV(alphas=(0.001,0.1,1),cv=5),\n",
        "          RandomForestRegressor(n_estimators=10),\n",
        "          GradientBoostingRegressor(n_estimators=30),\n",
        "          SVR(),\n",
        "          ElasticNet(alpha=0.001,max_iter=10000),\n",
        "          XGBRegressor()]\n",
        "\n",
        "def R2(model, x_train, x_test, y_train, y_test):\n",
        "  \"\"\" Ajusta o modelo aos dados de treinamento (x_train e y_train).\n",
        "      Computa o Coeficiente de Determinação para a predição realizada\n",
        "      por aquele modelo para os dados de teste: uma predição perfeita\n",
        "      possui score 1.0\n",
        "  \"\"\"\n",
        "  model_fitted = model.fit(x_train,y_train)\n",
        "  y_pred = model_fitted.predict(x_test)\n",
        "  score = r2_score(y_test, y_pred)\n",
        "  return score\n",
        "\n",
        "# efetua o ajuste dos modelos e exibe o resultado.\n",
        "for name,model in zip(names,models):\n",
        "  score = R2(model,x_train, x_test, y_train, y_test)\n",
        "  print('{}: {:.6f}, {:.4f}'.format(name,score.mean(),score.std()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnCJ8ziOGvLL",
        "colab_type": "text"
      },
      "source": [
        "## [Passo 04] Otimização de Hiperparâmetros\n",
        "Nessa etapa, procederemos com uma busca em grade para realizar a otimização dos hiperparâmetros. O método de regressão escolhido é o SVR, assim os seguintes parâmetros formarão a grade de busca:\n",
        "* Tipo de kernel, *linear* ou *RBF*;\n",
        "* C, o peso da penalidade de erro; e\n",
        "* Gamma, o parâmetro do kernel RBF.\n",
        "\n",
        "Tenha paciência ao executar a célula seguinte. Como se trata de uma busca exaustiva com validação cruzada (**3-fold cross validation**), é normal que o resultado demore um pouco para ser produzido. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qCBwgCfGzDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define os parâmetros da busca exaustiva: todas as combinações\n",
        "parameters = {\n",
        "    'kernel': ['linear', 'rbf' ],\n",
        "        'C' : [0.1  , 0.5 ,0.9,1,5],\n",
        "    'gamma' : [0.001, 0.01,0.1,1]\n",
        "}\n",
        "\n",
        "# busca exaustiva (grid search) com 3-fold cross validation\n",
        "print(\"starting exhaustive search... (please wait)\")\n",
        "model = GridSearchCV( SVR(), param_grid=parameters, cv=3 )\n",
        "model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-SvrnX--1Dv",
        "colab_type": "text"
      },
      "source": [
        "Agora podemos visualizar os hiperparâmetros que produziram os melhores resultados. Compare o valor do novo score obtido após a otimização com o obtido anteriormente: um salto de aproximadamente *0,517* para *0,618* ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "embdrYL2IbCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Optimal parameter list:', model.best_params_)\n",
        "print('Optimal model:', model.best_estimator_)\n",
        "print('Optimal R2 value (train):', model.best_score_)\n",
        "\n",
        "# verifica o score nos dados de teste\n",
        "y_pred = model.predict(x_test)\n",
        "best_score_test = r2_score(y_test, y_pred)\n",
        "print('Optimal R2 value (test) :', best_score_test )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_2QyfCTBzv1",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, podemos plotar um gráfico para comparar a previsão produzida pelo modelo com o resultado ideal, já conhecido nos dados de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkZ__6XBJbJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ln_x_test = range(len(x_test))\n",
        "y_predict = model.predict(x_test)\n",
        "\n",
        "plt.figure(figsize=(16,8), facecolor='w')\n",
        "# valores originais, conhecidos\n",
        "plt.plot (ln_x_test, y_test, 'r-', lw=2, label=u'Value')\n",
        "# valores previstos pelo modelo após a busca por parâmetros\n",
        "plt.plot (ln_x_test, y_predict, 'g-', lw = 3, label=u'Estimated value of the SVR algorithm, $R^2$=%.3f' %\n",
        "(best_score_test))\n",
        "\n",
        "# plotagem\n",
        "plt.legend(loc ='upper left')\n",
        "plt.grid(True)\n",
        "plt.title('Boston Housing Price Forecast (SVR)')\n",
        "plt.xlim(0, 101)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}