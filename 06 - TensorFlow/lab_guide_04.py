# -*- coding: utf-8 -*-
"""Lab Guide 04.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I4iOya-SPuhR8qMOXVjJXP9zEwj48_zZ

# Lab Guide 04 - TensorFlow 2.x
Nesse experimento, iremos trabalhar com os conceitos básicos da API Python do TensorFlow 2.x (ou seja, 2.0 ou superior) com Python 3, a qual já vem convenientemente instalada de antemão pelo ambiente do Google Colab.

Essa é uma atividade extensa, portanto recomendamos usar o índice à esquerda do Colab para navegação devido à quantidade de saídas geradas nas diversas células de código. Além disso, esteja ciente de que talvez seja preciso reiniciar o Ambiente de Execução deste notebook durante a **Atividade 4**.

Ao completar essas tarefas, você:
* entenderá a operação da API;
* estará familiarizado com o uso de tensores e sua aritmética; e
* terá experimentado as novidades introduzidas com a versão 2.0 do TensorFlow.

**NOTA:** esse *Lab Guide* é baseado no material da Huawei, sendo disponibilizado para uso exclusivo pelos alunos do curso e não deve ser divulgado sem autorização prévia e expressa.

## [Atividade 1] Tensores

Cada tensor possui uma série de propriedades, destacando-se:
 - **dtype**, o tipo de dados armazenado internamente. Geralmente são usados números de ponto-flutuante, porém há diversos outros tipos suportados;
 - **rank**, também conhecido como *ordem* ou *grau* do tensor, diz quantas dimensões o tensor possui. Isso equivale ao número de índices necessário para  acessar cada elemento armazenado em um tensor;
 - **shape**, o formato interno, ou seja, as dimensões desse tensor como arranjo multidimensional; e
 - **device**, o nome do dispositivo no qual esse tensor será produzido. 
 

Analogamente às linguagens de programação, há dois tipos de tensores no TensorFlow:
 - **constantes**. possuem dimensões fixas e os valores armazenados também são fixos desde a sua criação. Geralmente são usados para hiperparâmetros e dados estruturados que devam ser protegidos contra alteração;
 - **variáveis**. também possuem dimensões fixas, porém os valores armazenados podem ser alterados. Usados para armazenar peso e outros dados que mudam com o decorrer de um treinamento ou ativação.

### 1.1 *Criando Tensores Constantes*
Há diversas formas de criar um tensor constante em TensorFlow, das quais se destacam as seguintes funções:
 - **tf.constant()**, cria um tensor a partir de um arranjo multidimensional;
 - **tf.zeros()** **tf.zeros_like()**, as quais criam vetores povoados apenas com *0*;
 - **tf.ones()** e **tf.ones_like()**, as quais criam vetores povoados apenas com *1*;
 - **tf.fill()**, que cria um tensor com valores definidos pelo usuário;
 - **tf.random**, pacote com funções para criar tensores a partir de uma distriuição conhecida; e
 - **tf.convert_to_tensor()**, que converte vários tipos de objetos Python em tensores, geralmente subclasses de [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor).

#### 1.1.1 **tf.constant()**
Essa função recebe como entrada:
 - *value*, geralmente uma lista Python;
 - *dtype* (opcional), informa o tipo de dado armazenado no tensor, fazendo com que os valore sejam convertidos para o tipo desejado. Se não for informado, esse tipo é inferido a partir de value; 
 - *shape* (opcional), define a forma do tensor, de modo que a entrada é reformulada para ; e
 - *name* (opcional) o *nome* da constante. Útil para depuração e visualização do grafo.

Veja a [documentação da função tf.constant()](https://www.tensorflow.org/api_docs/python/tf/constant) para maiores detalhes.
"""

import tensorflow as tf

# cria uma matriz 2x2 a partir de uma lista com 4 elementos
const_a = tf.constant([[1, 2], [3, 4]], shape=[2,2], dtype=tf.float32)
print( "const_a:\n", const_a)

# note-se que a dimensão da lista é indiferente, desde que esta contenha a
# quantidade correta de elementos armazenados
const_b = tf.constant([[1, 2, 3, 4]],shape=[2,2], dtype=tf.float32)
const_c = tf.constant([ 1, 2, 3, 4] ,shape=[2,2], dtype=tf.float32)
const_d = tf.constant([[1], [2], [3], [4]],shape=[2,2], dtype=tf.float32)

# assim, todas as constantes são equivalentes
print("\nequivalentes:");
print(const_b)
print(const_c)
print(const_d)

"""Agora vamos visualizar as propriedades do nosso tensor:"""

# o método numpy() é bastante útil
print("valor do tensor: ", const_a.numpy())
print("\ttipo de dado armazenado:", const_a.dtype)
# há duas formas de acessar o rank: via tf.rank() ou pelo tamanho do shape
print("\trank:", tf.rank(const_a).numpy())
print("\trank:", len(const_a.shape))
print("\tshape:", const_a.shape)
# dispositivo associado, o qual pode ser None
print("\tdispositivo associado:", const_a.device)

"""#### 1.1.2 **tf.zeros()** e **tf.ones()**

As funções [tf.zeros()](https://www.tensorflow.org/api_docs/python/tf/zeros) e [tf.ones()](https://www.tensorflow.org/api_docs/python/tf/ones) são semelhantes, recebendo como argumentos:
 - *shape*, o formato do tensor. Uma lista ou tupla de inteiros, ou um tensor 1D do tipo int32;
 - *dtype* (opcional), define o tipo armazenado pelo tensor resultante. O tipo de dados padrão é *tf.float32*; e
 - *name* (opcional), o nome do tensor.
"""

# cria uma matriz 2x2 preenchida com zeros
zeros_a = tf.zeros( shape=[2,2], dtype=tf.int32, name='zeros_a' )
print(zeros_a)

# cria uma matriz 2x4 preenchida com uns
ones_a = tf.ones( shape=[2,4], dtype=tf.float32, name='zeros_a' )
print(ones_a)

"""#### 1.1.3 **tf.zeros_like()** e **tf.ones_like()**

As funções [tf.zeros_like()](https://www.tensorflow.org/api_docs/python/tf/zeros_like) e [tf.ones_like()](https://www.tensorflow.org/api_docs/python/tf/ones_like) são semelhantes às funções *tf.zeros()* e *tf.ones()*. Essas funções criam um tensor de zeros e uns, respectivamente, com *shape* e *dtype* idênticos aos de outro tensor.

Assim, essas funções recebem como argumentos:
 - *input*, um tensor;
 - *dtype* (opcional), define o tipo armazenado pelo tensor resultante. O tipo de dados padrão é igual ao de *input*; e
 - *name* (opcional), o nome do tensor.
"""

zero_a2 = tf.zeros_like(const_a)
print(zero_a2)

zero_a3 = tf.zeros_like(const_a,dtype=tf.int16,name='zero3')
print(zero_a3)

"""#### 1.1.4 **tf.fill()**
Cria um vetor preenchido com um determinado valor. Assim, recebe os seguintes parâmetros:
 - *dims*, o mesmo que o parâmetro *shape* das funções anteriores. Suporta os tipos *int32* e *int64*;
 - *value*, o valor que preenche o tensor criado. Note-se que o tipo de dados é deduzido a partir deste valor; e 
 - *name* (opcional), o nome do tensor.

**IMPORTANTE:** [tf.fill()](https://www.tensorflow.org/api_docs/python/tf/fill) avalia o grafo em tempo de execução, suportando *shape dinâmico* com base em outros tensores.
"""

filled_ei  = tf.fill( [3,2], 6)
filled_ef = tf.fill( [3,2], 6.0)
print(filled_ei)
print(filled_ef)

"""1.1.5 módulo **tf.random**

É possível fazer um embaralhamento ao longo da primeira dimensão usando a função [tf.random.shuffle()](https://www.tensorflow.org/api_docs/python/tf/random/shuffle). Essa operação é útil em diversos cenários, como divisão de datasets, por exemplo. Essa função recebe os seguintes argumentos:
 - *value*, o tensor a ser embaralhado;
 - *seed* (opcional), valor inteiro Python, usado para permitir a reprodução de uma sequência pseudoaleatória. Vide [tf.random.set_seed(()](https://www.tensorflow.org/api_docs/python/tf/random/set_seed) para maiores detalhes; e
 - *name* (opcional), nome do tensor.
"""

crows = tf.constant( [[1,2],[3,4],[5,6]] )

# necessário para um resultado previsível
tf.random.set_seed(5)
shuff = tf.random.shuffle(crows, seed=66 )
print('original:\n', crows)
print('\nembaralhado:\n', shuff)

"""As funções [tf.random.uniform()](https://www.tensorflow.org/api_docs/python/tf/random/uniform) e [tf.random.normal()](https://www.tensorflow.org/api_docs/python/tf/random/normal)geram valores aleatórios obedecendo às distribuições uniforme e normal (Gaussiana), respectivamente.

A função *tf.random.normal()*, ilustrada no exemplo a seguir, recebe os seguintes argumentos:
 - *shape*, shape do tensor a ser produzido;
 - *mean* (opcional), média da distribuição normal. O valor padrão é *0.0*;
 - *stddev* (opcional), desvio padrão da distribuição normal. O valor padrão é *1.0*;
 - *dtype* (opcional), o tipo de dados do tensor. Seu valor padrão é *tf.dtypes.float32*;
 - *seed* (opcional), semente aleatória, útil para reprodutibilidade de resultados. Seu valor padrão é *None*, indicando que não será usado; e
 - *name* (opcional), o nome do tensor.
"""

tf.random.set_seed(1);
random_e = tf.random.normal([5,5],mean=0,stddev=1.0, seed = 1)
#View the created data.
random_e.numpy()

"""#### 1.1.5 *tf.convert_to_tensor()*
A sua principal função é converter listas de objetos *Python* e arrays *numpy* em tensores. Além disso, essa função também suporta o tipo *Tensor* e escalares Python.

A função [tf.convert_to_tensor()](https://www.tensorflow.org/api_docs/python/tf/convert_to_tensor) recebe os seguintes argumentos:
 - *value*, um objeto convertível em tensor;
 - *dtype* (opcional), o tipo de elemento armazenado no tensor criado. O valor padrão é *None*, indicando que o tipo deve ser inferido a partir do tipo de *value*;
 - *dtype_hint* (opcional), usado quando dtype é None; e
 - *name* (opcional).
"""

# uma lista 
list_f = [1,2,3,4,5,6]
type(list_f)

# conversão
tensor_f = tf.convert_to_tensor(list_f, dtype=tf.float32)
tensor_f

"""### 1.2 Criando Tensores Variáveis
Variáveis são operadas usando a classe [tf.Variable](https://www.tensorflow.org/api_docs/python/tf/Variable), que representa tensores. Necessariamente, o valor inicial de uma variável deve ser especificado.

O valor de suas instâncias pode ser modificando ao executar uma operação aritmética. Além disso, os valores das variáveis podem tanto ser lidos quanto modificados.

Vejamos um exemplo simples:
"""

# o valor inicial precisa ser especificado 
v = tf.Variable(1.) 
print(v)

# o valor pode ser substituído usando o método assign()
v.assign(3.1415)
print(v)

"""Vejamos como criar uma variável a partir de uma constante:"""

var_1 = tf.Variable(tf.ones([2,3]))
var_1

"""Ler seu valor é bem simples, basta usar o método *read_value()*"""

# leitura
print("var_1 = ",var_1.read_value())

"""Alterar seu valor também é simples, podem é preciso ter cuidado. Usar o operador de atribuição **=** sobrescreve o tipo em Python, sendo mais seguro usar o método *assign()*"""

var_1.assign( [ [1.,2,3], [4.,5,6] ] )
var_1

"""Por fim, é possível utilizar operações diretamente no tensor. Por exemplo, usaremos o método *assign_add()* para acumular valores adiconados ao tensor."""

# adiciona um tensor de mesma dimensão
var_1.assign_add( tf.ones([2,3]) )
print('var_1 = ', var_1.read_value())

"""## [Atividade 2] Recorte e Indexação de Tensores
Os elementos de um tensor podem ser acessados convenientemente de diversas formas via Python. Nessa seção veremos como utilizar os operadores de recorte e indexação para esse propósito. 

Primeiro criemos um tensor relativamente simples, inicializado de forma aleatória para fins de testarmos esses operadores:
"""

""" Criemos um tensor possui 4 dimensões:
     - a dimensão mais externa é 5, indicando que temos 5 tensores de rank menor.
       Isso equivale a uma coleção com 5 imagens;
     - após isso, temos duas dimensões internas de 32 x 32. Assim, cada uma das
       5 imagens possui 32 pixels de largura por 32 de altura; e
     - por fim, cada pixel é armazenado em RGB com 3 componentes.    
"""
tensor_h = tf.random.normal([5,32,32,3])
print( tensor_h )
print( tensor_h.shape )

"""### 2.1 Recorte de Tensores
Os principais métodos de recorte recorrem aos operadores que o Python já provê para acessar listas multidimensionais:
 1. É possível usar valores específicos como `tensor[i]` ou intervalos como ```tensor[inicio:fim]``` para criar recortes;
 2. É possível usar o índice **:** para selecionar todos os elementos em uma dimensão;
 3. Opcionalmente, é possível definir passos para extrair uma sub-amostragem a partir de intervalos
  - `tensor[inicio:fim:passo]` faz com que o recorte seja formado no intervalo, porém salteando *passo* elementos. 
  - `tensor[::passo]` faz o mesmo, porém considerando todo o intervalo.
  - consequentemente `tensor[::-1]` extrair recorte a partir do último elemento.
 4. Por fim, `tensor[...]` indica um recorte de qualquer tamanho.

O exemplo a seguir extrai um recorte que contém a primeira imagem do tensor:
"""

# o operador acessa todo os intervalos da imagem (altura, largura, canal)
print( tensor_h[0, :, :, :] )

"""Assim, o exemplo a seguir extrai um recorte intervalando de 2 em 2 imagens:"""

# o operador ... omite os índices no restante do tensor
print( tensor_h[::2,...] )

"""Por fim, o exemplo a seguir extrai o canal R das imagens 0 e 3:"""

# o operador ... omite os índices no restante do tensor
print( tensor_h[::3, ..., 0 ] )

"""### 2.2 Indexação de Tensores
O acesso a um elemento específico do tensor é feito utiliza tantos índices quanto o *rank* de um tensor, o que equivalente à quantidade de elementos no seu *shape*.

Como no exemplo anterior, temos:
"""

# há 4 dimensões nesse tensor, portanto devemos usar 4 índices
print( tensor_h.shape )

# obtemos o pixel [3,32] no 2º canal da 1ª imagem 
print( tensor_h[0][2][31][1] )

"""#### 2.2.1 **tf.gather()**
A função [tf.gather()](https://www.tensorflow.org/api_docs/python/tf/gather) é útil para extrair índices que não são consecutivos a partir de um eixo, agrupando índices quaisquer naquele eixo. Essa função recebe os seguintes argumentos:
 - *params*, tensor de entrada;
 - *indices*, identifica quais índices dos dados de entrada devem ser extraídos;
 - *axis*, dimensão no tensor de entrada que será extraída; e
 - *batch_dims*, o número de dimensões no lote. Deve ser menor ou igual a **rank(indices)**; e
 - *name* (opcional).

No exemplo a seguir, são agrupadas a 1ª, a 4ª e a 2ª imagens do tensor com shape (5,32,32,3):
"""

# agrupa a 1ª, 4ª e 2ª imagens do tensor com shape (5,32,32,3)
print(tf.gather(tensor_h,axis=0,indices=[0,3,1],batch_dims=1))
print(tensor_h.shape)

"""#### 2.2.2 **tf.gather_nd()**
Semelhante a *tf.gather()*, porém [tf.gather_nd()](https://www.tensorflow.org/api_docs/python/tf/gather_nd) agrupa recortes de um tensor com shape especificado por indices, onde:
 - *params* é o tensor de entrada;
 - *indices* é um tensor k-1 de índices em *params*, de modo que cada elemento define um recorte de *params*;
 - *batch_dims*, opcional. Igual a 0 por padrão
"""

# tensor contém 2 matrizes
tensor_g = tf.constant( [ 
                         [[1., 2.], [3., 4.]],
                         [[5., 6.], [7., 8.]]
                        ] )

# 1) agrupa a 2ª linha da 1ª matriz com a 1ª linha da 2ª matriz
output = tf.gather_nd( tensor_g, indices = [[1], [0]],batch_dims=1 )
print(output)

# 2) agrupa a 2ª matriz com a 1ª
output2 = tf.gather_nd( tensor_g, indices = [[[1], [0]]] )
print()
print(output2)

"""## [Atividade 3] Redimensionando Tensores
A função [tf.shape()](https://www.tensorflow.org/api_docs/python/tf/shape) retorna um objeto [tf.Tensor](https://www.tensorflow.org/api_docs/python/tf/Tensor).

Por sua vez, o atributo *shape* e o método [get_shape()](https://www.tensorflow.org/api_docs/python/tf/Tensor#get_shape) de um tensor são equivalentes. Ambos retornam instâncias de [TensorShape](https://www.tensorflow.org/api_docs/python/tf/TensorShape).
"""

# muito embora esse shape seja conhecido, em algoritmos do mundo real
# utilizam shapes arbitrários que podem variar de acordo com aplicação
# e com as limitações do hardware (batch size, canais, etc). Assim, é
# importante saber acessar o formato do tensor
const_d_1 = tf.constant([[1, 2, 3, 4]],shape=[2,2], dtype=tf.float32)

# shape do tensor criado acima
print(const_d_1.shape)
print(const_d_1.get_shape())
# a saída é um tensor, pouco útil para acesso direto
print(tf.shape(const_d_1))

"""### 3.1 **tf.reshape()**
A função [tf.reshape()](https://www.tensorflow.org/api_docs/python/tf/reshape) é responsável por reordenar os elementos do tensor para que estes formem outra estrutura lógica, a qual afeta a indexação e o recorte. Essa função recebe os seguintes argumentos:
 - *tensor*, o tensor de entrada;
 - *shape*, o leiaute desejado;
 - *name* (opcional);

Assim, essa operação retorna num novo tensor com os valores do tensor oferecido como entrada, porém com um shape diferente. Essa é uma operação eficiente.
"""

# cria uma matriz 2 x 3
it = tf.constant([[1,2,3],[4,5,6]])
print(it)

# que contém tantos elementos quanto uma matriz 3 x 2,
# portanto o redimensionamento é permitido
print('\n', tf.reshape(it, (3,2)))

# da mesma forma, o redimensionamento pode resultar em um
# vetor de 6 elementos
print('\n', tf.reshape(it, (6)))

"""### 3.2 Adicionando Dimensões
A função [tf.expand_dims()](https://www.tensorflow.org/api_docs/python/tf/expand_dims) insere um novo eixo em tensores. Na prática, retorna um novo tensor com uma nova dimensão de tamanho 1, antes de um eixo desejado. Essa função tem como entrada:
 - *input*, tensor de entrada;
 - *axis*, eixo no qual será criada a nova dimensão. Caso seja negativo, a dimensão será criada após o eixo identificado, seguindo a lógica do Python. Assim, dado um tensor de *rank* D, esse argumento deve estar entre `-(D+1)` e `D`;
 - *name* (opcional).
"""

# cria um tensor 100 x 100 x 3, o que seria uma imagem RGB
img = tf.random.normal([100,100,3], seed=1)
print("shape original: ",img.shape)

print( "shape estendido (axis = 0):",
       tf.expand_dims(img, axis=0).shape)
print( "shape estendido (axis = 1):",
       tf.expand_dims(img, axis=1).shape)
print( "shape estendido (axis = 2):",
       tf.expand_dims(img, axis=2).shape)
print( "shape estendido (axis = 3):",
       tf.expand_dims(img, axis=3).shape)
print()

# agora eixos negativos
print( "shape estendido (axis = -1):",
       tf.expand_dims(img, axis=-1).shape)
print( "shape estendido (axis = -2):",
       tf.expand_dims(img, axis=-2).shape)
print( "shape estendido (axis = -3):",
       tf.expand_dims(img, axis=-3).shape)
print( "shape estendido (axis = -4):",
       tf.expand_dims(img, axis=-4).shape)

"""### 3.3 Removendo Dimensões
A função [tf.squeeze()](https://www.tensorflow.org/api_docs/python/tf/squeeze) remove dimensões de tamanho 1, recebendo os seguintes argumentos:
 - *input*, tensor de entrada;
 - *axis*, eixo no qual será criada a nova dimensão. Caso seja negativo, a dimensão será criada após o eixo identificado, seguindo a lógica do Python. Assim, dado um tensor de *rank* D, esse argumento deve estar entre `-(D+1)` e `D`;
 - *name* (opcional).
"""

# cria um tensor 1 x 100 x 100 x 3, o que seria um batch com 
# uma única imagem RGB
img = tf.random.normal([1, 100,100,3], seed=1)
print( "shape original: ",img.shape)
print( "shape reduzido:", tf.squeeze(img).shape)

# agora um tensor com uma propriedade extra por pixel
img2 = tf.random.normal([1,100,100,3,1], seed=1)
print()
print( "shape original 2: ",img2.shape)
# essa chamada remove todas as dimensões de valor 1
print( "shape reduzido:", tf.squeeze(img2).shape)
# é possível remover apenas a dimensão desejada
print( "shape reduzido no eixo 4:", tf.squeeze(img2, [4]).shape)
# é possível remover apenas a dimensão desejada
print( "shape reduzido no eixo 0:", tf.squeeze(img2, [0]).shape)

"""### 3.4 Transposição de Tensores
A função [tf.transpose()](https://www.tensorflow.org/api_docs/python/tf/transpose) é capaz de computar o transposto e conjugado de um tensor. Essa função recebe os seguintes parâmetros:
 - *a*, um tensor;
 - *perm* (opcional), uma permutação das dimensões de *a*;
 - *conjugate* (opcional), booleano, se definido como *`True`*, a função retorna o tensor conjugado do transposto. Útil para lidar com tensores contendo números complexos; e
 - *name* (opcional).
"""

t = tf.constant([1,2,3,4,5,6],shape=[2,3])

print("tensor original:\n", t.numpy())
print("shape original: ", t.shape)
tt = tf.transpose(t)

print("tensor transposto:\n", tt.numpy())
print("shape do transposto:", tt.shape)

ctt = tf.transpose(t,conjugate=True)
print("tensor transposto conjugado:\n", ctt.numpy())
print("shape do transposto conjugado:", ctt.shape)

"""Caso o usuário necessite, é possível permutar a entrada, prodizindo resultado mais interessante e útil para lidar com tensores de alta dimensionalidade. Isso permite transposições em dimensões internas, por exemplo, o que é útil para compatibilizar dados que usam diferentes convenções de alinhamento."""

# tensor 4 x 100 x 200 x 3, representa 4 imagens RGB de 100 x 200
batch = tf.random.normal([4,100,200,3])
print("shape original:", batch.shape)

# transpõe as imagens do batch, ou seja, troca altura e largura.
# a permutação original [0,1,2,3] é alterada para [0,2,1,3], o
# que afeta as dimensões internas do tensor e preserva valores
tbatch = tf.transpose(batch,[0,2,1,3])
print("shape transposto:", tbatch.shape)

"""### 3.5 **Broadcast** (Difusão)
Também é possível difundir dados de um tensor de baixa dimensionalidade para formar um *shape* de dimensão maior. Para tanto, usemos a função [tf.broadcast_to()](https://www.tensorflow.org/api_docs/python/tf/broadcast_to):
 - *input*, o tensor de entrada com baixa dimensionalidade;
 - *shape*, o shape de destino, que deve ser compatível com o shape do tensor *input*; e
 - *name* (opcional).
"""

low = tf.constant( [1,2,3,4,5,6] )
print("dados originais:\n", low.numpy())

high = tf.broadcast_to(low,shape=[4,6])
print("dados após difusão:\n", high.numpy())

"""Também é importante salientar que, durante a operação entre dois tensores, o TensorFlow automaticamente dispara o mecanismo de broadcast da mesma maneira que NumPy:"""

# uma matriz
a = tf.constant( [[ 0, 0, 0],
                  [10,10,10],
                  [20,20,20],
                  [30,30,30]] )

# um vetor-linha é compatível. Note-se que um vetor-coluna 
# não "casa" com o formato interno ao tensor a
b = tf.constant([1,2,3])
print(a + b)

"""### 3.6 **Operações Aritméticas**

Há diversas operações aritméticas simples suportadas pelo TensorFlow na forma de funções:
 - **tf.add()**;
 - **tf.subtract()**;
 - **tf.multiply()**;
 - **tf.divide()**;
 - **tf.math.log()**;
 - **tf.log()**;
 - e [muitas outras](https://www.tensorflow.org/api_docs/python/tf/math/).
"""

import tensorflow as tf

a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
print("a + b = ", tf.add(a, b))
print("a - b = ", tf.subtract(a, b))

"""#### 3.6.1 **Multiplicação Matricial**
Basta usar a função [tf.matmul()](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul), como visto no Lab Guide anterior:
"""

# multiplicação
tf.matmul(a,b)

"""#### 3.6.2 **Coletando Estatísticas de Tensores**

Há diversas funções para o cálculo de estatísticas a partir de tensores:
 - **tf.reduce_min()**, **tf.reduce_max()** e **tf.reduce_mean()** calculam os valores mínimo, máximo e médio em um tensor, respectivamente;
 - **tf.argmax()** e **tf.argmin()** calculam as posições onde ocorrem os valores máximo e mínimo de um tensor, respectivamente;
"""

ten = tf.constant([[1,3,2],[2,5,8],[7,5,9]])

print("input tensor:\n", ten.numpy())
print("minimum:", tf.reduce_min(ten).numpy())
print("maximum:", tf.reduce_max(ten).numpy())
print("mean:", tf.reduce_mean(ten).numpy())

max1 = tf.argmax( ten, axis=0)
max2  = tf.argmax( ten, axis=1)
print("index of the maximum value, by column:", max1.numpy())
print("index of the maximum value, by row:", max2.numpy())

"""- **tf.equal()** e o operador de comparação **===** verificam se dois tensores são iguais, elemento por elemento;
 - **tf.unique()** remove elementos duplicados em um tensor 1D;
"""

a = tf.constant( [[1,3,1], [3,1,3], [1,3,1]] )
b = tf.transpose(a)

print( "a == b? ", tf.equal(a, b) )
print( a == b )

# converte a para um vetor 1D
va = tf.reshape(a, [9])
print( "\nva:", va.numpy() )

# computa os valores únicos e suas ocorrências em va, estas como índices
u, ids =  tf.unique( va )
print( "valores únicos:", u.numpy() )
print( "índices desses valores em va:", ids.numpy() )

"""- [tf.math.in_top_k()](https://www.tensorflow.org/api_docs/python/tf/math/in_top_k) calcula se o valor previsto (*prediction*) é igual a um alvo (*target*), retornando um tensor booleano.

#### 3.6.3 **Operações Aritméticas Baseadas na Dimensão**

Há uma série de operações que reduzem as dimensões de um tensor, as quais são identificadas pelo prefixo **tf.reduce_** e que podem ser realizadas em elementos multidimensionais de um tensor, tais como:
 - [tf.math.reduce_min()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_min), [tf.math.reduce_max()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_max) e [tf.math.reduce_mean()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean), já vistas;
 - [tf.math.reduce_sum()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum), adição;
 - [tf.math.reduce_prod()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_prod), multiplicação;
 - [tf.math.reduce_all()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_all), conjunção lógica (AND);
 - [tf.math.reduce_any()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_any), disjunção lógica (OR); e
 - [tf.math.reduce_logsumexp()](https://www.tensorflow.org/api_docs/python/tf/math/reduce_logsumexp ), disjunção lógica (OR).

Todas essas operações são bastante semelhantes e possuem *aliases* ("apelidos") no módulo raiz **tf**, portanto analisemos os argumentos da função **tf.math.reduce_sum()**, equivalente a **tf.reduce_sum()**:
 - *input_tensor*, tensor numérico a ser "reduzido";
 - *axis*, dimensões a serem reduzidas. Caso não seja fornecido, todas as dimensões são reduzidas;
 - *keepdims*, se True, mantém dimensões reduzidas de tamanho 1; e
 - *name* (opcional).
"""

# soma todos os valores em um tensor
print(tf.reduce_sum(a))
# produz um tensor de rank 2, mas de dimensões unitárias
print(tf.reduce_sum(a,keepdims=True))

"""Vejamos um exemplo mais complexo:"""

rs1 = tf.constant([1,2,3,4,5,6],shape=[2,3])
print("dados originais:\n", rs1.numpy())
print("soma de todos os elementos (axis = None):",tf.reduce_sum(rs1,axis=None).numpy())
print("soma por coluna (axis = 0):\n",tf.reduce_sum(rs1,axis=0).numpy())
print("soma por linha (axis = 1):\n",tf.reduce_sum(rs1,axis=1).numpy())
print("soma por linha (axis = 1), mantendo as dimensões reduzidas:\n"
      , tf.reduce_sum(rs1,axis=1,keepdims=True).numpy())

"""### 3.7 **Concatenação e Particionamento de Tensores**

Dentre as operações com tensores, note-se que também possível juntar tensores de modo a formar um tensor de maior dimensões. Além disso, o caminho inverso também é possível, ou seja, particionar um tensor em outros.

#### 3.7.1 **Concatenando Tensores**

Tanto **tf.concat()** quanto **tf.stack()** concatenam tensores. A função [tf.concat()](https://www.tensorflow.org/api_docs/python/tf/concat) concatena dois vetores ao longo de um dimensão, recebendo como argumentos:
 - *values*, uma lista de tensores ou um único tensor;
 - *axis*, eixo ao longo do qual concatenar os tensores; e
 - *name* (opcional).
"""

# 1) matrizes
t1 = [[1, 2, 3], [4, 5, 6]]
t2 = [[7, 8, 9], [10, 11, 12]]
print( 't1 =', t1 )
print( 't2 =', t2 )

# concatena as linhas
print("concatenated data (rows):")
print( tf.concat([t1, t2], 0).numpy() )

# concatena as colunas
print("concatenated data (columns):")
print( tf.concat([t1, t2], 1).numpy() )


# 2) 'lotes' de imagens (batches)
b1 = tf.random.normal([ 4,100,100,3])
b2 = tf.random.normal([40,100,100,3])

# concatena os dois lotes em um só
bc = tf.concat([b1,b2],axis=0)
print("size of the concatenated data:",bc.shape)

"""A função [tf.stack()](https://www.tensorflow.org/api_docs/python/tf/stack) faz algo semelhante, porém empacota os tensores, produzindo tensor de rank maior. Para tanto, recebe os seguintes argumentos:
 - *values*, uma lista de tensores de mesmo shape e tipo de dados;
 - *axis*, eixo ao longo do qual empilhar os tensores. Também aceita valores negativos; e
 - name (opcional).
"""

x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
# combina os tensores, produzindo uma matriz
print( tf.stack([x, y, z]).numpy())

# produz a matriz transposta, empilhando coluna por coluna
tf.stack([x, y, z], axis=1).numpy()

"""#### 3.7.2 **Particionando Tensores**

Analogamente à concatenação e empilhamento, um tensor também pode ser subdividido em partições menores usando as funções **tf.split()** e **tf.unstack()**.

A função [tf.split()](https://www.tensorflow.org/api_docs/python/tf/split) particiona um tensor em uma lista de sub-tensores. Essa função recebe os seguintes argumentos:
 - *value*, tensor a ser subdividido;
 - *num_or_size_splits*, pode ser tanto um inteiro indicando o número de subdivisões ao longo do eixo *axis*, quanto um Tensor ou lista Python de 1D dimensão contendo os tamanhos de cada vetor ao o longo do eixo;
 - *axis*, dimensão  ao longo a qual subvididir;
 - *num* (opcional), especifica o número de saídas quando não pode ser inferido a partir do *shape* de size_splits; e
 - *name* (opcional).
"""

import tensorflow as tf

ori = tf.random.normal([10,100,100,3])
print("shape original:",ori.shape)
s1 = tf.split(ori, num_or_size_splits=5,axis=0)
print("shape da subdivisão com m_or_size_splits=5:", s1[0].shape)

# divide o batch em 3 partes, que somam 10
s2 = tf.split(ori, num_or_size_splits=[3,5,2],axis=0)
print("dados resultantes da subdivisão para num_or_size_splits=[3,5,2]:\n",
s2[0].shape,
s2[1].shape,
s2[2].shape)

"""A função [tf.unstack()](https://www.tensorflow.org/api_docs/python/tf/unstack) subdivide um tensor com base em uma dimensão específica. Essa função recebe os seguintes argumentos:"""

# cria uma pilha de tensores
s1 = tf.constant([1,2,3])
s2 = tf.constant([4,5,6])
s3 = tf.constant([7,8,9])
ss = tf.stack([s1, s2, s3],axis=0)
print("dimensão dos tensores empilhados = ", ss.shape)
print("pilha resultante:\n", ss.numpy())

# desfaz a pilha de tensores, produzindo uma lista
l = tf.unstack( ss,axis=0)
print("\ndimensões dos tensores desempilhados de ss:")
for x in l:
  print( "shape:", x.shape, 'valor:', x.numpy())

"""### 3.8 **Ordenando Tensores**
A seleção de valores com base na ordem e ordenação em geral são ferramentas fundamentais para resolver diversos problemas. Assim, as principais funções de ordenação do TensorFlow são:
 - [tf.sort()](https://www.tensorflow.org/api_docs/python/tf/sort), que ordena um tensor ao longo de um eixo e retorna o tensor ordenado;
 - [tf.argsort()](https://www.tensorflow.org/api_docs/python/tf/argsort), semelhante a *tf.sort()*, porém retorna os índices que produzem um tensor ordenado ao longo de um eixo; e
 - [tf.math.top_k()](https://www.tensorflow.org/api_docs/python/tf/math/top_k), que retorna os valores e os respectivos índices para os *k* maiores elementos para a última dimensão do tensor.

Os argumentos mais comuns dessas funções são:
 - *values*, o tensor de entrada;
 - *axis* (opcional), eixo a ser ordenado. Caso não seja fornecido, assume valor -1, que corresponde ao último eixo;
 - *direction*, o sentido da ordenação *'ASCENDING'* ou *'DESCENDING '*. O valor padrão é *'ASCENDING'*;
 - *stable*, um booleano, quando *True* indica que um algoritmo de ordenação estável deve ser usado. Isso garante que elementos iguais no tensor de entrada não serão permutados pela ordenação, o que pode gerar ruído no sentido lógico em algoritmos. Algoritmos não estáveis geralmente são mais eficientes e provavelmente serão implementados pela API do TensorFlow, portanto *stable=True* é uma escolha mais adequada para compatibilidade futura;
 - *name* (opcional).
"""

# a) lida com a ordenação de uma matriz
mat = tf.constant( [[3,4], [1,2],[2,1]] )
print("matriz inicial:", mat.numpy())
print("matriz ordenada:", tf.sort(mat,axis=0).numpy())

# cria um tensor como permutação de 10 inteiros
tin = tf.random.shuffle(tf.range(10))
print("\ntensor inicial:", tin.numpy())

# ordenação
ordered1 = tf.sort(tin, direction="ASCENDING")
print("\ntensor ordenado em ordem ascendente:", ordered1.numpy())
ordered2 = tf.argsort(tin,direction="ASCENDING")
print("indices dos elementos em ordem ascendente:",ordered2.numpy())

# valores e índices
vk, idxk = tf.math.top_k( tin, k=3 )
print( "\nos 3 maiores valores: ", vk.numpy())
print( "que estão nas posições:",idxk.numpy())

vk2, idxk2 = tf.math.top_k( tin, k=3, sorted=False )
print( "\nos 3 maiores valores, em ordem crescente: ", vk2.numpy())
print( "que estão nas posições:",idxk2.numpy())

"""## [Atividade 4] Eager Execution no TensorFlow 2.x
Há dois modos principais de execução no framework TensorFlow desde a versão 2.0, que introduziu um modo interativo ("ansioso"), o qual parece ser mais favorável aos novatos nessa intrincada API.

O modo **Eager Execution** é um tipo de programação *imperativa* e que funciona de maneira similar ao Python ou C++. Nesse modelo de programação, o sistema informa imediatamente sobre o resultado de uma operação tão logo esta seja concluída. Esse é o modo padrão do TensorFlow 2.0 e que temos usado até agora neste notebook do Colab.

O **Graph Mode**, por sua vez, é o modo disponível desde a versão 1.0 do TensorFlow e que assume uma perspectiva mais formal, de programação *declarativa*. Nesse modo, um *grafo computacional* é construído antes que uma *sessão* seja criada e habilitada para executá-lo, para que só então os dados sejam alimentados nesse processo "burocrático" para produzir um resultado.

Esses dois modos serão comparados a seguir usando uma simples multiplicação.

### 4.1 **Eager Execution**
"""

# cada tensor é associado imediatamente ao seu valor
x = tf.ones((2, 2), dtype=tf.dtypes.float32)
y = tf.constant([[1, 2],
                 [3, 4]], dtype=tf.dtypes.float32)

# o resultado de operações está sempre disponível.
# Além disso, o programador sequer habilita uma sessão.
z = tf.matmul(x, y)
print(z)

"""### 4.2 **Graph Mode**"""

# usemos a sintaxe
import tensorflow as tf
import tensorflow.compat.v1 as tf1
from tensorflow.python.framework.ops import disable_eager_execution

# desabilitemos o modo padrão
disable_eager_execution()

# criemos o grafo
a = tf.ones((2, 2), dtype=tf.dtypes.float32)
b = tf.constant([[1, 2],
                 [3, 4]], dtype=tf.dtypes.float32)
c = tf.matmul(a, b)

# sem uma sessão, o valor de c não pode ser calculado
print( 'sem a sessão, c = ', c);

# habilitando uma sessão, executamos o grafo que produz c
with tf1.Session() as sess:
    print(sess.run(c))
    sess.close()

"""A vantagem do eager execution está na facilidade de visualizar o resultado, assim como na disponibilidade de funções nativas em Python.

Por exemplo, é possível usar condicionais mais facilmente com o *eager mode*, permitindo que fluxos de controle dinâmicos gerem valores NumPy acessíveis por um tensor, o que dispensa o uso de operadores como **tf.cond** e **tf.while**, os quais são providos em *graph mode*.

**NOTA:** de agora em diante, vá no menu do Colab e reinicie o ambiente de execução. Caso isso não seja feito, é provável que os próximos códigos continuem sendo executados em *graph mode*.
"""

import tensorflow as tf

try:
  if (not tf.executing_eagerly()):
    print('Por favor, reinicie o ambiente de execução deste notebook.')
    print('1) Vá no menu \"Ambiente de execução\"')
    print('2) Clique em \"Reiniciar ambiente de execução\"')
    exit(-1)
except Exception as ex:
  print(ex);


# executa normalmente, de forma imperativa
thre_1 = tf.random.uniform([], 0, 1)
x = tf.reshape(tf.range(0, 4), [2, 2])
print(thre_1)
if thre_1.numpy() > 0.5:
    y = tf.matmul(x, x)
else:
    y = tf.add(x, x)

"""## [Atividade 5] AutoGraph no TensorFlow 2.x
Uma das novidades do TensorFlow 2.x é o uso de um decorador para comentar funções: [@tf.function](https://www.tensorflow.org/api_docs/python/tf/function), o qual pode ser usado como qualquer outro decorador. No caso, **@tf.function** fará com que a função decorada seja compilada na forma de um grafo otimizável, de modo que possa executar mais eficientemente por aceleração em uma GPU ou TPU.

Com efeito, a função decorada se torna uma operação do TensorFlow e que pode ser executada diretamente para retornar um único valor. Todavia, note-se que a execução subjacente previne que valores intermediários de variáveis sejam acessados diretamente.
"""

@tf.function
def simple_nn_layer(w,x,b):
    # esse valor é desconhecido internamente
    print(b)
    return tf.nn.relu(tf.matmul(w, x)+b)

# instancia as entradas da camada
w = tf.random.uniform((3, 3))
x = tf.random.uniform((3, 3))
b = tf.constant(0.5, dtype='float32')

# captura o resultado, já que o valor de 'b' é "ofuscado"
c = simple_nn_layer(w,x,b)
print( '\nc = ', c.numpy() )

"""A próxima célula de código Python efetua uma comparação do desempenho: (a) eager execution *versus* (b) graph mode com AutoGraph."""

import timeit

# camada convolucional: eager execution
CNN_cell = tf.keras.layers.Conv2D(filters=100,kernel_size=2,strides=(1,1))

# conversão automática em grafo nessa função
@tf.function
def CNN_fn(image):
    return CNN_cell(image)

# a imagem de entrada não importa: usaremos zeros
image = tf.zeros([100, 200, 200, 3])

# executa uma primeira vez
CNN_cell(image)
CNN_fn(image)

# benchmark usando 30 execuções
runs = 30
print("tempo da camada CNN, em eager execution:\n", timeit.timeit(lambda: CNN_cell(image), number=runs))
print("tempo da camada CNN, em graph mode:\n", timeit.timeit(lambda: CNN_fn(image), number=runs))

"""Os valores podem variar de plataforma para plataforma, mas os testes no Colab mostraram que o *graph mode* é executado *1,37* vezes mais rápido. 

Dependendo do ambiente hospedeiro e da complexidade do grafo, é provável que a execução em *graph mode* seja muito mais eficiente. Assim, o simples uso do decorador **@tf.function** em funções é um mecanismo conveniente de otimização.


---

#[Atividade 5] Conhecendo os Principais Módulos da API TensorFlow 2.x
Agora que você está familiarizado com a manipulação de tensores e as principais operações disponíveis para esse tipo de dados, iremos introduzir os principais módulos presents na API do TensorFlow 2:
 - **tf.data**, para lidar com datasets, incluindo leitura a partir de diversas fontes (memória, arquivos CSV e arquivos TFRecord) e *data augmentation*;
 - **tf.image**, para processamento de imagens, incluindo redimensionamento, rotação, detecção de arestas, bem como transformações sobre luma e chroma;
 - **tf.gfile**, lida com arquivos e diretórios, permitindo leitura, escrita e renomeação;
 - **tf.keras**, uma API de alto nível para construir e treinar modelos de *deep learning*. *NOTA:* essa API não é necessariamente idêntica ao módulo Keras. Além disso, a compatibilidade entre diferentes versões do TensorFlow e versões específicas do Keras é um assunto que requer cuidado;
 - e muitas outras, como **tf.distribute**, usada para executar computação em múltiplos dispositivos.

Os experimentos realizados nessa atividade possuem como foco o módulo [tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras), pois visa fixar o conhecimento sobre modelos de deep learning.

### 5.1 - Modelos Sequenciais
A forma mais comum de construir um modelo consiste em empilhar camadas a partir de um modelo [tf.keras.Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential), como no exemplo a seguir.
"""

import tensorflow as tf
import tensorflow.keras.layers as layers

# cria um modelo como sequência de camadas
model = tf.keras.Sequential()

# empilha camadas no modelo
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(10, activation='relu'))

"""Contudo, esse um modelo sequencial não é adequado para lidar com as seguintes situações:
 - múltiplas entradas;
 - múltiplas saídas;
 - compartilhamento de camadas;
 - lidar com topologias/fluxos não lineares ou sequenciais, tais como uma ramificação múltipla ou uma conexão residual (como na arquitetura ResNet).

#### 5.1.1 - Visualizando um Modelo
Há várias formas de visualizar a estrutura de um modelo no TensorFlow, porém as mais comuns são via um resumo textual, que é mais simples, e a plotagem de um gráfico mais sofisticado.
"""

# é preciso construir o modelo antes de acessar o sumário
# para tanto, o método build, passando o shape do batch. 
# usaremos (None, 32) com um coringa
model.build((None, 32))

# imprime o sumário do modelo
print( model.summary() )

"""Já a plotagem para uma imagem pode ser feita de forma conveniente usando a função utilitária
[keras.utils.vis_utils.plot_model()](https://keras.io/api/utils/model_plotting_utils/#plotmodel-function)
"""

from keras.utils.vis_utils import plot_model

plot_model(model, to_file='sequential.png', show_shapes=True, show_layer_names=True)

"""### 5.2 Modelos Funcionais
Modelos funcionais são construídos de forma mais complexa e declarativa do que modelos criados via **tf.keras.Sequential**. Isso é feito com base nas classes **tf.keras.Input** and **tf.keras.Model**, porém isso tem um efeito positivo, pois podem contornar as limitações de um modelo sequencial.

Por exemplo, em modelos funcionais, é possível usar as variáveis como entradas ao mesmo tempo ou em diferentes fases do processamento do grafo. Além disso, os dados podem ser gerados como saída em diversas fases do processamento. Assim, é preferível utilizar modelos funcionais caso mais de uma saída seja necessária.
"""

# usa a saída 
x = tf.keras.Input(shape=(32,))
h1 = layers.Dense(32, activation='relu')(x)
h2 = layers.Dense(32, activation='relu')(h1)
y = layers.Dense(10, activation='softmax')(h2)

# cria o modelo funcional, de 'x' para 'y'
funcModel = tf.keras.models.Model(x, y)

# exibe o sumário
funcModel.summary()

"""### 5.3 Construindo Camadas de uma Rede Profunda
O módulo **tf.keras.layers** contém funcionalidades que facilitam a configuraçã ode camadas de uma rede neural. As classes mais comumente usadas são:
 - [tf.keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), uma camada totalmente conectada (fully connected);
 - [tf.keras.layers.Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) e [tf.keras.layers.Conv3D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3D), que constrem convoluções sobre imagens 2D e 3D, respectivamente;
 - [tf.keras.layers.MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) e [tf.keras.layers.AveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D), para camadas de pooling usadas em CNNs;
 - [tf.keras.layers.RNN](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RNN), para camada de uma rede neural recorrente;
 - [tf.keras.layers.LSTM](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM) e [tf.keras.layers.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell), as quais constroem uma camada de uma rede LSTM e uma unidade LSTM, respectivamente;
 - similarmente, as classes [tf.keras.layers.GRU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU) e [tf.keras.layers.GRUCell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell) constroem uma camada de uma rede GRU e uma unidade GRU, respectivamente;
 - [tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding), a qual transforma índice, i.e., inteiros positivos, em um vetor de tamanho fixo. Contudo, um *embedding* só pode ser usado como a primeira camada de um modelo; e
 - [tf.keras.layers.Dropout](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dropout), o qual constrói uma camada de *dropout* que desativa aleatoriamente de entrada em tempo de treinamento para ajudar a prevenir *overfitting*.

Os principais parâmetros de configuração nas camadas disponíveis no pacote **f.keras.layers** são:
 - **activation**, define a função de ativação daquela camada. Por padrão, o sistema não aplica uma função;
 - **kernel_initializer** e **bias_initializer**, ambos opcionais, que definem como os pesos iniciais dos *kernels* e dos *biases* são computados. O método padrão é o inicializador uniforme Glorot;
 - **kernel_regularizer** e **bias_regularizer**, ambos opcionais, que defime os esquemas de regularização aplicados aos pesos (kernel e bias) da camada, tais como a regularização L1 ou L2. O sistema não aplica uma regularização por padrão.

#### 5.3.1 **tf.keras.layers.Dense**
Os principais parâmetros de configuração desse tipo de camada são:
 - *units*, número de neurônios;
 - *activation*, função de ativação;
 - *use_bias*, indica se usa ou não um bias, que são ativados por padrão;
 - *kernel_initializer*, inicializador de pesos do kernel;
 - *bias_initializer*, inicializador de pesos para do *bias*;
 - *kernel_regularizer* e *bias_regularizer*, esquema de regularização aplicável ao kernel e ao bias, respectivamente;
 - *activity_regularizer*, função de regularização aplicada à saída desta camada, i.e., à sua ativação;
 - *kernel_constraint* e *bias_constraint*, restrições aplicadas aos pesos do kernel e ao bias, respectivamente.
"""

""" Cria uma camada com 32 neurônios com ativação sigmóide.
    Note-se que a ativação tanto pode ser uma string como a
    própria função Python.
"""
layers.Dense(32, activation='sigmoid')
layers.Dense(32, activation=tf.sigmoid)

# definindo um inicializador de kernel 
layers.Dense(32, kernel_initializer=tf.keras.initializers.he_normal)
# habilitando regularização L2 com fator 0.05
layers.Dense(32, kernel_regularizer=tf.keras.regularizers.L2(0.05))

"""#### 5.3.2 **tf.keras.layers.Conv2D**
Os principais parâmetros de configuração de uma convolução 2D são:
 - *filters*, o número de kernels de convolução. Quanto maior, mais neurônios e conexões são criados para as dimensões de saída da camada;
 - *kernel_size*, largura e altura do kernel. Geralmente quanto maior, menor a saída;
 - *strides*, o deslocamento entre ;
 - *padding*, política de preenchimento das bordas. Quando o *padding* é definido como **'valid'**, apenas convoluções no domínio válido são efetuadas, ou seja, a borda externa não é processada por falta de informação. Caso o padding seja definido como **'same'**, a convolução preserva a borda e geralmente resulta em uma saída de mesma dimensão da entrada;
 - *activation* (opcional), função de ativação desta camada. Há diversas opções padrões no módulo [tf.keras.activations](https://www.tensorflow.org/api_docs/python/tf/keras/activations);
 - *data_format*, define o formato das dimensões nas entradas qual formato usar.
  **channels_first**, valor padrão, dita o shape organizado como *(batch_size, channels,height, width)*. Já **channels_last** corresponde ao shape *(batch_size, height, width, channels)*.
"""

layers.Conv2D(64,[1,1],2,padding='same',activation="relu")

"""#### 5.3.3 **tf.keras.layers.MaxPool2D** e **tf.keras.layers.AveragePooling2D**
Os principais argumentos que configuram esses operadores de pooling autoexplicativos são:
 - *pool_size=(2,2)*, dimensões do kernel a ser agrupado. Quanto maior, menor a saída produzida. Por exemplo, uma matriz 2 x 2 produz uma saída com metade do tamanho da entrada em ambas as dimensões. Caso um valor inteiro seja especificado, é usado em todas as dimensões; e
 - *strides=None*, deslocamento entre ativações consecutivas. Usado para aumentar a distância entre os elementos da entrada 2D.

O exemplo a seguir constrói um operador que causa deslocamentos de 2 pixels em uma dimensão e 1 pixel na outra. Com efeito, a largura da saída é a metade da original, enquanto a altura da saída é menor do que a entrada por apenas 1 pixel. Confira mais exemplos e detalhes na documentação para [tf.keras.layers.MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) e [tf.keras.layers.AveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D).
"""

layers.MaxPooling2D(pool_size=(2,2),strides=(2,1))

"""#### 5.3.4 **tf.keras.layers.LSTM** e **tf.keras.layers.LSTMCell**"""

import numpy as np
from tensorflow.keras import layers

# cria uma rede com uma camada LSTM
inputs = tf.keras.Input(shape=(3, 1))
lstm = layers.LSTM(1, return_sequences=True)(inputs)
model_lstm_1 = tf.keras.models.Model(inputs=inputs, outputs=lstm)

# cria uma segunda rede para comparar com a primeira
inputs = tf.keras.Input(shape=(3, 1))
lstm = layers.LSTM(1, return_sequences=False)(inputs)
model_lstm_2 = tf.keras.models.Model(inputs=inputs, outputs=lstm)

# cria uma sequência de entrada t1, t2, and t3
# as LSTMs serão alimentadas com essa sequência
data = [ [[0.1], [0.2], [0.3]] ]
print(data)

# a sequência de saída completa
print("saída quando return_sequences=True :", model_lstm_1.predict(data))
# a última célula da sequência de saída
print("saída quando return_sequences=False:", model_lstm_2.predict(data))

"""Já a classe LSTMCell é a implementação de uma unidade da camada LSTM."""

# cria uma LSTM
tf.keras.layers.LSTM(4, return_sequences=True)

# cria uma unidade LSTMCell em uma RNN
x = tf.keras.Input(shape=(3, 1))
y = layers.RNN(layers.LSTMCell(4))(x)
rnn = tf.keras.Model(x, y)

# teste com dados aleatórios anteriores
print("saída da RNN:", rnn.predict(data))

"""### 5.4 Treinando e Avaliando o Modelo
Veremos a seguir uma visão geral das várias etapas envolvidas no ciclo de vida de um modelo já criado, que compreende as seguintes etapas: *compilação*, *treinamento*, *avaliação do desempenho* e *inferência*.

#### 5.4.1 Compilando o Modelo
Após construir um modelo, é necessário compilá-lo para configurar o seu processo de aprendizagem. Isso é encapsulado pelo método [compile()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) da classe **tf.keras.Model**, cujos principais argumentos são:
 - *optimizer='rmsprop'*, string identificando o otimizador;
 - *loss=None*, a função de perda usada para guiar o processo de otimização;
 - *metrics=None*, lista de critérios de avaliação do modelo durante treino e teste. Geralmente se usa *metrics=['accuracy']*. Várias conversões internas são feitas automaticamente para que o método de computação da acurácia seja adequado à rede do seu modelo; e
 - *loss_weights=None*, argumento muito conveniente para modelos que lidam com múltiplas tarefas e que devem produzir múltiplas saídas. Assim, *loss_weights* define um peso para cada uma dessas saídas quando o treinamento estiver otimizando o *loss* global composto.
"""

import tensorflow as tf

# cria um modelo muito simples
model = tf.keras.Sequential()
model.add( tf.keras.layers.Dense(10, activation='softmax'))

"""
Configura a aprendizagem do modelo:
 - Adam como otimizador;
 - categorical_crossentropy como função de perda;
 - categorical_accuracy como método de avaliação de desempenho.
"""
model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
             loss=tf.keras.losses.categorical_crossentropy,
             metrics=[tf.keras.metrics.categorical_accuracy])

"""#### 5.4.2 Treinando um Modelo
O processo de treinamento do modelo é disparado pelo método [fit()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) da classe **tf.keras.Model**. Esses são os principais argumentos do método **fit()**:
 - *x=None*, dados de entrada;
 - *y=None*, dados-alvo da saída desse modelo;
 - *batch_size=None*, tamanho do lote;
 - *epochs=1*, épocas, i.e., número de vezes que o modelo percorre um dataset por treinamento;
 - *verbose=1*, modo de verbosidade, que influencia as mensagens impressas durante o treinamento.  0 = silencioso, 1 = barra de progresso, 2 = uma linha por época;
 - *callbacks=None*, lista opcional de callbacks, úteis para customizar o treinamento;
 - *validation_split=0.0*, número entre 0 e 1 usado para informar qual fração dos dados de treinamento será usada para validação;
 - *validation_data=None*, dataset de validação. Quando informado, *validation_split* é ignorado;
 - *shuffle=True*, indica se os dados devem ser embaralhados antes de cada iteração. Inválido se *steps_per_epoch* não for None;
 - *initial_epoch=0*, diz em qual época o treinamento inicia. Útil para continuar o processo de treino;
 - *steps_per_epoch=None*, número total de passos (batches de amostras) entes de considerar que uma época do treinamento foi finalizada. Seu valor depende do tamanho do dataset e do tamanho do batch (*batch_size*); e
 - *validation_steps*, número total de passos (batches de amostras) de validação validar antes de acabar uma época. Válido apenas quando *steps_per_epoch* é fornecido.
"""

import numpy as np

# produz um dataset aleatório
train_x = np.random.random((1000, 36))
train_y = np.random.random((1000, 10))
val_x = np.random.random((200, 36))
val_y = np.random.random((200, 10))

# invoca o método de treinamento
model.fit(train_x, train_y, epochs=10, batch_size=100,
          validation_data=(val_x, val_y))

"""Treinamento com datasets de grande escala requer uma pipeline adequada. Por exemplo, pode ser necessário repetir um *dataset*, para que seja revisto várias vezes durante o treinamento: isso pode ser feito usando o método [repeat()](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#repeat) da classe **Dataset**. Por outro lado, também é possível um controle sobre como os datasets de treinamento e de validação são usados a cada época. Isso é ilustrado no exemplo a seguir."""

# dataset de treinamento
dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))
dataset = dataset.batch(32)
dataset = dataset.repeat() # repete indefinidamente

# dataset de validação
val_dataset = tf.data.Dataset.from_tensor_slices((val_x, val_y))
val_dataset = val_dataset.batch(32)
val_dataset = val_dataset.repeat() # repete indefinidamente

"""
O processo de treinamento procede assim:
 - são 10 épocas sobre todo o dataset;
 - a cada época, 30 passos são usasos para treino e 3 para teste.
"""
model.fit(dataset, epochs=10, steps_per_epoch=30,
          validation_data=val_dataset, validation_steps=3)

"""#### 5.4.3 Tratando Eventos de Treinamento: Funções Callback
O processo de treinamento é longo, de modo que ocorrerão diversos eventos que podem (e devem) ser tratados para que o programador consiga customizar o processo desejado. Exemplo de eventos incluem: salvamento de uma cópia de segurança (*checkpoint*), alteração do *learning rate*, parada prematura e exportação de dados de progresso para o TensorBoard.

Desse modo, o programador pode especificar diferentes funções para tratar esses eventos. Essas funções são chamadas de *callbacks* e permitem estender o comportamento do modelo durante a fase de treinamento. Por conveniência, a API do TensorFlow oferece diversas funções pré-prontas no módulo [tf.keras.callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks), porém o programador também tem a liberdade de definir suas próprias funções.

Os principais *callbacks* no módulo **tf.keras.callbacks** são:
 - **ModelCheckpoint**, que periodicamente salva modelos;
 - **LearningRateScheduler**, que altera o *learning rate* dinamicamente durante o treinamento, permitindo que o usuário defina o controle fino dessa política no otimizador;
 - **EarlyStopping**, que faz com que o treinamento seja abortado;
 - **TensorBoard**, para exportação e visualização de informações sobre o progresso e os resultados do treinamento por meio da ferramenta [TensorBoard](https://www.tensorflow.org/tensorboard/get_started).

O exemplo a seguir definirá funções callback customizadas
"""

# hiperparâmetros do treinamento
Epochs = 10

# escalonador de learning rate customizado por época
def lr_Scheduler(epoch):
    if epoch > 0.9 * Epochs:
        lr = 0.0001
    elif epoch > 0.5 * Epochs:
        lr = 0.001
    elif epoch > 0.25 * Epochs:
        lr = 0.01
    else:
        lr = 0.1
        
    print(lr)
    return lr
            

callbacks = [
    # condições para abortar o treinamento
    tf.keras.callbacks.EarlyStopping(
        # métrica usada para checar se o modelo estagnou o desempenho
        monitor='val_loss',
        # limiar para determinar se o desempenho melhorou
        min_delta=1e-2,
        # número de épocas para aguardar enquanto o modelo não melhora o desempenho
        patience=2),
    
    # criação de checkpoints
     tf.keras.callbacks.ModelCheckpoint(
        # onde salvar: path usa a época
        filepath='testmodel_{epoch}.h5',
        # manter todos os modelos ou salvar apenas o melhor?
        save_best_only=True,
        # qual a métrica monitorada
        monitor='val_loss'),
    
    # instala a política de alteração learning rate.
    tf.keras.callbacks.LearningRateScheduler(lr_Scheduler),
    
    # configura o uso de TensorBoard
    tf.keras.callbacks.TensorBoard(log_dir='./logs')
]

# inicia o treinamento
model.fit(train_x, train_y, batch_size=16, epochs=Epochs,
         callbacks=callbacks, validation_data=(val_x, val_y))

"""#### 5.4.4 Avaliando o Modelo
A avaliação do modelo pode ser feita usando o método [evaluate()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#evaluate). Note-se que o processamento é realizado por lotes, de acordo com o parâmetro **batch_size**, como no exemplo a seguir.
Como resultado, teremos métricas como *loss* e acurácia.
"""

# cria amostras de entrada
test_x = np.random.random((1000, 36))
test_y = np.random.random((1000, 10))

# o tipo dos objetos em y é consistente com aqueles em x
model.evaluate(test_x, test_y, batch_size=32)

"""**NOTA:** o exemplo acima é apenas ilustrativo,sendo propositalmente aleatório, portanto não espere que o modelo consiga aprender um padrão consistente.

#### 5.4.5 Usando o Modelo para Inferência
Basta invocar o médoto [predict()](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict) do modelo com um tensor de entrada representando uma amostra ou um lote de amostras.
"""

# entrada aleatória
pre_x = np.random.random((10, 36))

# inferência simples
result = model.predict(test_x,)

# visualização do resultado
print(result)

"""### 5.5 Salvando e Carregando Modelos
O treinamento de um modelo de *deep learning* é uma tarefa delicada e computacionalmente complexa que pode demorar horas, dias ou mesmo semanas. Isso posto, é importante saber como manter seus modelos à disposição no disco e como carregá-los depois.

#### 5.5.1 Salvando e Carregando o Modelo Completo
Salvar o modelo é relativamente simples. Todavia, esse processo perpetua todos os pesos ao longo da rede e também a estrutura do grafo computacional, *i.e.*, a descrição das camadas e conexões que compõem a rede.

Além disso, o arquivo também conterá outras informações que permitirão continuar o treinamento do modelo, caso necessário.
"""

import numpy as np
from os import path, mkdir

if not path.exists('./model'):
  mkdir('./model')

# salva o modelo completo
model.save('./model/the_best_model.h5')

# -----------------------------------------------------------------
# o carregamento do modelo é simples: uma chamada
new_model = tf.keras.models.load_model('./model/the_best_model.h5')
new_prediction = new_model.predict(test_x)


# teste de similaridade entre os objetos considerando uma margem
np.testing.assert_allclose(result, new_prediction, atol=1e-6)

"""#### 5.5.2 Salvando e Carregando apenas os Pesos da Rede
Caso o nome do arquivo de pesos possua o sufixo **.h5** ou **.keras**, o modelo será salvo para o formato HDF5 (*Hierarchical Data Format*) binário e eficiente. Caso contrário, pode-se salvar o modelo como um checkpoint do TensorFlow.
"""

# salvamento do modelo em duas opções
model.save_weights('./model/model_weights') #checkpoint
model.save_weights('./model/model_weights.h5')

# carregamento dos pesos em duas opções
model.load_weights('./model/model_weights') # checkpoint
model.load_weights('./model/model_weights.h5')